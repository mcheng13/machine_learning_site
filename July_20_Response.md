## Response for Monday, July 20
A. When it comes to the stock market, investors of all different types all have the same goal: to beat the market. There are countless strategies that can be used and almost anything can be an indicator if used correctly. Is there a way that I can create a model to accurately predict stock price movement or beat the market? If this is possible, it could be a very important investing tool to find good stocks to invest in that result in a better return. One large issue is the fact that this model would not be able to account for any real life events, as the data it uses can only be from the past. These events can also be specific to one stock or they can affect the entire global market, such as the crash that happened as a result of COVID-19. Another issue is that typically, algorithms work better for large-cap stocks, but penny stocks, or those worth under $10 per share, are harder to predict. Of course, this could actually lead to me focusing on penny stocks, as they have more potential for growth and have less algorithm attention on them from large firms. Lastly, there are many different factors that affect stock prices including humans and human sentiment, trading algorithms, general market sentiment, price valuations, earnings reports, current events, fundamentals, and more. These factors, although creating more of a challenge, also offer many different approaches to predicting price movement. Price history could be used to predict future movement, market sentiment could be analyzed and used to predict the movement of the market as a whole, more mentions of a certain stock in the news could lead to changes in the stock's price, and much more. 

B. The data used for could be the price history of a stock (like KO, Coca Cola, as they have a long history which could be helpful for training), but this is definitely not the only option. Data could also be news articles or headlines, as this could be collected to see if there is a correlation between mentions in the news and changes in stock prices. Another option I've considered is looking at earnings reports in the past and training a model to predict the reaction to an upcoming earnings report. There are likely many other options as well if none of these end up working.

C. The poster could be laid out showing first the price history of the chosen stock that the training was done on then a comparison between the prediction and the actual recent stock price change. This could also be done on multiple stocks side by side (preferable stocks that tend to have different behavior).

D. The cat and dog data was set up by first downloading the cat and dog images zip, then unzipping the file, and lastly separating the dataset into cat and dog source directories as well as training and testing directories.  
1.  The RMSprop optimizer was selected. This optimizer averages over mini-batches of data, but, in order to work for really large datasets, it keeps track of the moving average of the squared gradients for each weight and then divides by the root mean square. This means that adjacent mini-batches will be divided by similar numbers every time, instead of allowing for a lot of variation. When being compared to other choices, RMSProp scales the learning rate so that it can go through saddle points much faster. It also goes through almost the most optimal path when working with very large initial gradients and avoids overshooting.  
2.  The selected loss function is binary crossentropy, which is optimal for a binary classifier, like the current dataset. What it essentially does is predict the log probability of a data point belonging to one of the classes. The closer to 1, the higher the chance, and the closer to 0, the higher the chance it belongs to the other class. In order to penalize bad predictions, the negative log of the probability is used. As the probability of the data point's true class gets closer to zero, the loss increases exponentially.  
3.  What a metric does is it judges the performance of the model instead of evaluating it in training. In this case, the metric is accuracy, so that means our model is judged on how often it correctly predicts the class. This is important as it gives us a way to properly determine how good our model is in practice.  
4. This model had decent validation accuracy of about .85 at the eighth epoch. After this point the validation accuracy became stagnant and even decreased slightly, but the training accuracy continued to increase. Although the model became overfit after the third epoch, this difference only became significant after the eighth. Looking at the loss, the validation loss was relatively high at a minimum of .4 again at the eighth epoch. This time, the training loss continued to decrease while the validation accuracy increased rather sharply. Overall, the performance was decent, but it seems like it could be improved.  ![Unknown-9](https://user-images.githubusercontent.com/67922851/88001974-61701d80-cacf-11ea-8f5a-7bd5e01c59ae.png)  
![Unknown-10](https://user-images.githubusercontent.com/67922851/88001980-6339e100-cacf-11ea-8e8a-1bc71f13c6ec.png)   
5.  
![cat1](https://user-images.githubusercontent.com/67922851/88005603-c3348580-cad7-11ea-8875-caf46b6442a0.jpeg)
Predicted cat
![cat2](https://user-images.githubusercontent.com/67922851/88005605-c3cd1c00-cad7-11ea-80f9-20090c4f4ed9.jpeg)
Predicted dog
![cat3](https://user-images.githubusercontent.com/67922851/88005606-c465b280-cad7-11ea-9691-ddf209f41ceb.jpeg)
Predicted cat
![dog1](https://user-images.githubusercontent.com/67922851/88005608-c4fe4900-cad7-11ea-8d3a-34061e87bd8a.jpeg)
Predicted dog
![dog2](https://user-images.githubusercontent.com/67922851/88005610-c596df80-cad7-11ea-979b-9e048df98ebc.jpeg)
Predicted dog
![dog3](https://user-images.githubusercontent.com/67922851/88005612-c62f7600-cad7-11ea-95b8-d04df3b659bb.jpeg)
Predicted dog  

In practice, the model performed decently well, predicting five out of the six images correctly. It appears like dogs are labelled correctly for the most part, but cats are labelled as dogs semi-frequently. To fix this, maybe images could be zoomed in before testing, as zoomed in images seem to have a higher chance of misidentification, so if all images are zoomed in, then maybe the model could perform better. Other than that, maybe the images in the training set could include more variation? I am not sure what all of them are, as there are thousands of images, but ideally, the pictures would have animals of different sizes and colors from different angles. The more variation, I feel the more variation in validation images the model could handle.
